{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Linear Regression - Ridge Regression   \n",
    "-------------------------\n",
    "\n",
    "_Authors: Khal Makhoul, W.P.G.Peterson_  \n",
    "\n",
    "## Project Guide  \n",
    "\n",
    "--------------------\n",
    "- [Project Overview](#overview)  \n",
    "- [Data Review](#data)  \n",
    "- [Coding Ridge Regression](#code)\n",
    "- [Ridge Regression in `sklearn`](#sklearn)  \n",
    "\n",
    "<a id = \"overview\"></a>\n",
    "## Project Overview  \n",
    "-------------\n",
    "#### EXPECTED TIME 2.5 HRS  \n",
    "\n",
    "This assignment will test your ability to code your own version of ridge-regularized regression in `Python`. This assignment draws upon and presupposed the knowledge found in the lectures for Week 2. If ever a theoretical questions arises as to \"why\" we are doing something, please refer back to those lectures.  \n",
    "\n",
    "The assignment also builds upon the work performed in assignment 1 \"*Linear Regression - Least Squares*\". The data used will be the same. Though the last assignment tested your ability to read data into `Pandas` from a `.csv`. Those fundamental processes will not be directly tested here.\n",
    "\n",
    "In coding Ridge Regression you will be asked to:  \n",
    "- Mean center target variable and mean center / standardize observation  \n",
    "- Calculate Ridge Regression weights using linear algebra\n",
    "- Create a  hyperparameter tuning process   \n",
    "\n",
    "**Motivation**: Ridge Regression offers a way to mitigate some of the weaknesses of Least Squares Linear Regression to build more robust models.  \n",
    "\n",
    "**Objectives**: This assignmet will -  \n",
    "\n",
    "- Test `Python` competency and mathematical understandings of Ridge Regression\n",
    "- Begin to introduce the concept of hyper-parameter tuning  \n",
    "\n",
    "**Problem**: Using housing data, we will attempt to predict house price using living area with a regression model.  \n",
    "\n",
    "**Data**: Data comes from [Kaggle's House Prices Dataset](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data).  \n",
    "\n",
    "See above link for Description of data.\n",
    "\n",
    "<a id = \"data\"></a>  \n",
    "### Data Exploration\n",
    "\n",
    "Below provides a review of the \"Housing\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### This cell imports the necessary modules and sets a few plotting parameters for display\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (20.0, 10.0)  \n",
    "\n",
    "### Read in the data\n",
    "tr_path = 'data/Assignment 2 - datasets/train.csv'\n",
    "test_path = 'data/Assignment 2 - datasets/test.csv'\n",
    "\n",
    "data = pd.read_csv(tr_path)  \n",
    "\n",
    "### The .head() function shows the first few lines of data for perspecitve\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Which column has the most \"null\" values? assign name as string to ans1.\n",
    "### ### CAPITALIZATION/SPELLING MATTERS e.g. 'Street' != 'street'\n",
    "### How many nulls are in that column? assign number as int to ans2\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "null_columns=data.columns[data.isnull().any()]\n",
    "data[null_columns].isnull().sum()\n",
    "\n",
    "ans1 = 'PoolQC'\n",
    "ans2 = 1453"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 1",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data.plot('GrLivArea', 'SalePrice', kind = 'scatter', marker = 'x');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### In building regressions below, a subset of our data will be used.\n",
    "\n",
    "### Practice subsetting a DataFrame below.\n",
    "### Create a DataFrame only containing the \"Street\" and \"Alley\" columns from \n",
    "### the `data` DataFrame.\n",
    "\n",
    "### Assign to 'ans1'\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "ans1 = data[['Street','Alley']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 2",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<a id = \"code\"></a>\n",
    "### Coding Ridge Regression   \n",
    "\n",
    "#### Preprocessing\n",
    "Before implementing Ridge Regression, it is important to mean-center our target variable and mean-center and standardize observations. We will do this according to the following:  \n",
    "#### Mean Center Target\n",
    "$$y_{cent} = y_0 - \\bar{y}$$\n",
    "\n",
    "#### Standardize Observations\n",
    "$$X_{std} = \\frac{X_0-\\bar{X}}{s_{X}}$$\n",
    "\n",
    "Where $\\bar{X}$ is the sample mean of X and $s_{X}$ is the sample standard deviation of X.  \n",
    "\n",
    "NOTE: The sample standard deviation should be calculated with 0 \"Delta Degrees of Freedom\"\n",
    "\n",
    "#### Question 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Why are the centering / standardization transformations described above important for ridge regression?\n",
    "### 'a') Regression works best when values are unitless\n",
    "### 'b') The transformations makes the regression more interpretable\n",
    "### 'c') Ridge penalizes large coefficients; the transformations make the coefficients of similar scales\n",
    "### 'd') It isn't important\n",
    "\n",
    "### Assign character associated with your choice as a string to ans1\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "ans1 = 'c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 3",
     "locked": true,
     "points": "2",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### Question 4:\n",
    "$$X_{std} = \\frac{X_0-\\bar{X}}{s_{X}}$$  \n",
    "\n",
    "NOTE: The sample standard deviation should be calculated with 0 \"Delta Degrees of Freedom\".  \n",
    "If your answer does not match the example answer, check the default degrees of freedom in your standard deviation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Code a function called \"standardize\" \n",
    "### ACCEPT one input, a list of numbers\n",
    "### RETURN a list where those values have been standardized.\n",
    "\n",
    "### To standardize, subtract the mean of the list and divide by standard deviation.\n",
    "### Please use np.std for calculating standard deviation\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "def standardize(num_list):\n",
    "    \"\"\"\n",
    "    Standardize the given list of numbers\n",
    "    \n",
    "    Positional arguments:\n",
    "        num_list -- a list of numbers\n",
    "    \n",
    "    Example:\n",
    "        num_list = [1,2,3,3,4,4,5,5,5,5,5]\n",
    "        nl_std = standardize(num_list)\n",
    "        print(np.round(nl_std,2))\n",
    "        #--> np.array([-2.11, -1.36, -0.61, -0.61,  \n",
    "                0.14,  0.14,  0.88,  0.88,  0.88,\n",
    "                0.88,  0.88])\n",
    "    \n",
    "    NOTE: the sample standard deviation should be calculated with 0 \"Delta Degrees of Freedom\"\n",
    "    \"\"\"\n",
    "    \n",
    "    num_mean = sum(num_list) / len(num_list)\n",
    "    num_std = np.std(num_list)\n",
    "    num_list[:] = [x - num_mean for x in num_list]\n",
    "    num_list[:] = [x/num_std for x in num_list]\n",
    "    \n",
    "    return num_list\n",
    "\n",
    "#num_list = [1,2,3,3,4,4,5,5,5,5,5]\n",
    "#nl_std = standardize(num_list)\n",
    "#print(np.round(nl_std,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 4",
     "locked": true,
     "points": "20",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Below we will create a function which will preprocess  our data by performing:\n",
    "* mean subtraction from $y$,\n",
    "* dimension standardization for $x$.\n",
    "\n",
    "Both according to the equations set out below.\n",
    "\n",
    "#### Mean Center Target\n",
    "$$y_{cent} = y_0 - \\bar{y}$$\n",
    "\n",
    "#### Standardize Observations\n",
    "$$X_{std} = \\frac{X_0-\\bar{X}}{s_{X}}$$  \n",
    "\n",
    "NOTE: the sample standard deviation should be calculated with 0 \"Delta Degrees of Freedom\"  \n",
    "If your answer does not match the example answer, check the default degrees of freedom in your standard deviation function.\n",
    "\n",
    "#### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_regularization(data, y_column_name, x_column_names):\n",
    "    \"\"\"\n",
    "    Perform mean subtraction and dimension standardization on data\n",
    "        \n",
    "    Positional argument:\n",
    "        data -- a pandas dataframe of the data to pre-process\n",
    "        y_column_name -- the name (string) of the column that contains\n",
    "            the target of the training data.\n",
    "        x_column_names -- a *list* of the names of columns that contain the\n",
    "            observations to be standardized\n",
    "        \n",
    "    Returns:\n",
    "        Return a DataFrame consisting only of the columns included\n",
    "        in `y_column_name` and `x_column_names`.\n",
    "        Where the y_column has been mean-centered, and the\n",
    "        x_columns have been mean-centered/standardized.\n",
    "        \n",
    "        \n",
    "    Example:\n",
    "        data = pd.read_csv(tr_path).head()\n",
    "        prepro_data = preprocess_for_regularization(data,'SalePrice', ['GrLivArea','YearBuilt'])\n",
    "        \n",
    "        print(prepro_data) #-->\n",
    "                   GrLivArea  YearBuilt  SalePrice\n",
    "                0  -0.082772   0.716753     7800.0\n",
    "                1  -1.590161  -0.089594   -19200.0\n",
    "                2   0.172946   0.657024    22800.0\n",
    "                3  -0.059219  -1.911342   -60700.0\n",
    "                4   1.559205   0.627159    49300.0\n",
    "    \n",
    "    NOTE: The sample standard deviation should be calculated with 0 \"Delta Degrees of Freedom\"\n",
    "    \n",
    "    If your answer does not match the example answer,\n",
    "    check the default degrees of freedom in your standard deviation function.\n",
    "    \"\"\"\n",
    "    \n",
    "    y_mean = sum(data[y_column_name]) / len(data[y_column_name])\n",
    "    data[y_column_name] = data[y_column_name] - y_mean\n",
    "    \n",
    "    x_mean = []\n",
    "    x_std = []\n",
    "    for c in range(len(x_column_names)):\n",
    "        x_mean.append(sum(data[x_column_names[c]]) / len(data[x_column_names[c]]))\n",
    "        x_std.append(np.std(data[x_column_names[c]]))\n",
    "    \n",
    "    data[x_column_names] = data[x_column_names] - x_mean\n",
    "    data[x_column_names] = data[x_column_names] / x_std\n",
    "    \n",
    "    all_columns = x_column_names\n",
    "    all_columns.append(y_column_name)\n",
    "    \n",
    "    return data[all_columns]\n",
    "\n",
    "#data = pd.read_csv(tr_path).head()\n",
    "#prepro_data = preprocess_for_regularization(data,'SalePrice', ['GrLivArea','YearBuilt'])\n",
    "#print(prepro_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 5",
     "locked": true,
     "points": "20",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Next, you'll implement the equation for ridge regression using the closed form equation:  \n",
    "\n",
    "$$w_{RR}=(\\lambda+X^TX)^{-1}X^Ty$$  \n",
    "\n",
    "The function will be very similar to the function you wrote for Least Squares Regression with a slightly different matrix to invert.  \n",
    "\n",
    "NB: Many `numpy` matrix functions will be useful. e.g. `np.matmul`, `np.linalg.inv`, `np.ones`, `np.transpose`, and`np.identity`.\n",
    "\n",
    "The main change from Least Squares Regression is that $\\lambda$ is a parameter *we* must set. This is different from the $w$ parameters that we calculate from either closed form or approximation algorithms.  \n",
    "\n",
    "We will address tuning parameters such as $\\lambda$ in the next section.  \n",
    "\n",
    "#### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Code a function called \"ridge_regression_weights\"\n",
    "### ACCEPT three inputs:\n",
    "### Two matricies corresponding to the x inputs and y target\n",
    "### and a number (int or float) for the lambda parameter\n",
    "\n",
    "### RETURN a numpy array of regression weights\n",
    "\n",
    "### The following must be accomplished:\n",
    "\n",
    "### Ensure the number of rows of each the X matrix is greater than the number of columns.\n",
    "### ### If not, transpose the matrix.\n",
    "### Ultimately, the y input will have length n.\n",
    "### Thus the x input should be in the shape n-by-p\n",
    "\n",
    "### *Prepend* an n-by-1 column of ones to the input_x matrix\n",
    "\n",
    "### Use the above equation to calculate the least squares weights.\n",
    "### This will involve creating the lambda matrix---\n",
    "### ### a p+1-by-p+1 matrix with the \"lambda_param\" on the diagonal\n",
    "### ### p+1-by-p+1 because of the prepended \"ones\".\n",
    "\n",
    "### NB: Pay close attention to the expected format of the returned\n",
    "### weights. It is different / simplified from Assignment 1.\n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "def ridge_regression_weights(input_x, output_y, lambda_param):\n",
    "    \"\"\"Calculate ridge regression least squares weights.\n",
    "    \n",
    "    Positional arguments:\n",
    "        input_x -- 2-d matrix of input data\n",
    "        output_y -- 1-d numpy array of target values\n",
    "        lambda_param -- lambda parameter that controls how heavily\n",
    "            to penalize large weight values\n",
    "        \n",
    "    Example:\n",
    "        training_y = np.array([208500, 181500, 223500, \n",
    "                                140000, 250000, 143000, \n",
    "                                307000, 200000, 129900, \n",
    "                                118000])\n",
    "                                \n",
    "        training_x = np.array([[1710, 1262, 1786, \n",
    "                                1717, 2198, 1362, \n",
    "                                1694, 2090, 1774, \n",
    "                                1077], \n",
    "                               [2003, 1976, 2001, \n",
    "                                1915, 2000, 1993, \n",
    "                                2004, 1973, 1931, \n",
    "                                1939]])\n",
    "        lambda_param = 10\n",
    "        \n",
    "        rrw = ridge_regression_weights(training_x, training_y, lambda_param)\n",
    "        \n",
    "        print(rrw) #--> np.array([-576.67947107,   77.45913349,   31.50189177])\n",
    "        print(rrw[2]) #--> 31.50189177\n",
    "        \n",
    "    Assumptions:\n",
    "        -- output_y is a vector whose length is the same as the\n",
    "        number of observations in input_x\n",
    "        -- lambda_param has a value greater than 0\n",
    "    \"\"\"\n",
    "    \n",
    "    #Step 1\n",
    "    y_shape = output_y.shape    \n",
    "    try:\n",
    "        if y_shape[0] < y_shape[1]:\n",
    "            output_y.transpose()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    x_shape = input_x.shape\n",
    "    X = input_x.transpose() if x_shape[1] >= x_shape[0] else input_x\n",
    "    \n",
    "    #Step2\n",
    "    n = len(X)\n",
    "    X = np.hstack((np.ones((n,1), dtype=np.float64), X))\n",
    "    \n",
    "    #Step3\n",
    "    \n",
    "    xtx = X.T.dot(X)\n",
    "    xtx = (lambda_param * np.eye(X.shape[1])) + xtx\n",
    "    inv = np.linalg.inv(xtx)\n",
    "    xty = X.T.dot(output_y)\n",
    "    w_ls = inv.dot(xty)\n",
    "    \n",
    "    weights = np.array(w_ls)\n",
    "    return weights\n",
    "\n",
    "\n",
    "\n",
    "training_y = np.array([208500, 181500, 223500, \n",
    "                                140000, 250000, 143000, \n",
    "                                307000, 200000, 129900, \n",
    "                                118000])\n",
    "                                \n",
    "training_x = np.array([[1710, 1262, 1786, \n",
    "                        1717, 2198, 1362, \n",
    "                        1694, 2090, 1774, \n",
    "                        1077], \n",
    "                       [2003, 1976, 2001, \n",
    "                        1915, 2000, 1993, \n",
    "                        2004, 1973, 1931, \n",
    "                        1939]])\n",
    "\n",
    "lambda_param = 10\n",
    "        \n",
    "rrw = ridge_regression_weights(training_x, training_y, lambda_param)\n",
    "        \n",
    "print(rrw)\n",
    "print(rrw[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 6",
     "locked": true,
     "points": "15",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### Selecting the $\\lambda$ parameter\n",
    "\n",
    "For our final function before looking at the `sklearn` implementation of ridge regression, we will create a hyper-parameter tuning algorithm.  \n",
    "\n",
    "In ridge regression, we must pick a value for $\\lambda$. We have some intuition about $\\lambda$ from the equations that define it: small values tend to emulate the results from Least Squares, while large values will reduce the dimensionality of the problem. But the choice of $\\lambda$ can motivated with a more precise quantitative treatment.\n",
    "\n",
    "Eventually, we will look to choose the value of $\\lambda$ that minimizes validation error, which we will determine using $k$-fold cross-validation.\n",
    "\n",
    "For this example here, we will solve a simpler problem: Find a value that minimizes the of the list returned by a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### Example of hiden function below:\n",
    "\n",
    "### `hidden` takes a single number as a parameter (int or float) and returns a list of 1000 numbers\n",
    "### the input must be between 0 and 50 exclusive\n",
    "\n",
    "def hidden(hp):\n",
    "    if (hp<=0) or (hp >= 50):\n",
    "        print(\"input out of bounds\")\n",
    "    \n",
    "    nums = np.logspace(0,5,num = 1000)\n",
    "    vals = nums** 43.123985172351235134687934\n",
    "    \n",
    "    user_vals = nums** hp\n",
    "    \n",
    "    return vals-user_vals\n",
    "\n",
    "hidden(43.123985172351235134687934)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Run the above cell and test out the functionality of `hidden`. Remember it takes a single number, between 0 and 50, as an argument\n",
    "\n",
    "#### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Code a function called \"minimize\"\n",
    "### ACCEPT one input: a function.\n",
    "\n",
    "### That function will be similar to `hidden` created above and available for your exploration.\n",
    "### Like 'hidden', the passed function will take a single argument, a number between 0 and 50 exclusive \n",
    "### and then, the function will return a numpy array of 1000 numbers.\n",
    "\n",
    "### RETURN the value that makes the mean of the array returned by 'passed_func' as close to 0 as possible\n",
    "\n",
    "### Note, you will almost certainly NOT be able to find the number that makes the mean exactly 0\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "def minimize( passed_func):\n",
    "    \"\"\"\n",
    "    Find the numeric value that makes the mean of the\n",
    "    output array returned from 'passed_func' as close to 0 as possible.\n",
    "    \n",
    "    Positional Argument:\n",
    "        passed_func -- a function that takes a single number (between 0 and 50 exclusive)\n",
    "            as input, and returns a list of 1000 floats.\n",
    "        \n",
    "    Example:\n",
    "        passed_func = hidden\n",
    "        min_hidden = minimize(passed_func)\n",
    "        print(round(min_hidden,4))\n",
    "        #--> 43.1204 (answers will vary slightly, must be close to 43.123985172351)\n",
    "    \n",
    "    \"\"\"    \n",
    "    \n",
    "    smallest_mean = 0.0001\n",
    "    alpha = 0.0001\n",
    "    while True:\n",
    "        current_mean = np.mean(passed_func(alpha))\n",
    "        if current_mean <= smallest_mean:\n",
    "            break\n",
    "        else :\n",
    "            alpha = alpha + 0.0001\n",
    "    \n",
    "    return alpha\n",
    "\n",
    "passed_func = hidden\n",
    "min_hidden = minimize(passed_func)\n",
    "print(round(min_hidden,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 7",
     "locked": true,
     "points": "20",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "The above simulates hyper parameter tuning.  \n",
    "\n",
    "In the case of ridge regression, you would be searching lambda parameters to minimize validation error.  \n",
    "\n",
    "The `hidden` function would be analogous to the model building; the returned list analogous to residuals; and the mean of that list analogous to validation error.  \n",
    "\n",
    "See below for an example of using the functions built above that automatically performs hyper-parameter tuning using mean-absolute-deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def lambda_search_func(lambda_param):\n",
    "    \n",
    "    # Define X and y\n",
    "    # with preprocessing\n",
    "    df = preprocess_for_regularization(data.head(50),'SalePrice', ['GrLivArea','YearBuilt'])\n",
    "    \n",
    "    y_true = df['SalePrice'].values\n",
    "    X = df[['GrLivArea','YearBuilt']].values\n",
    "    \n",
    "    # Calculate Weights then use for predictions\n",
    "    weights = ridge_regression_weights(X, y_true, lambda_param )\n",
    "    y_pred = weights[0] + np.matmul(X,weights[1:])\n",
    "    \n",
    "    # Calculate Residuals\n",
    "    resid = y_true - y_pred\n",
    "    \n",
    "    # take absolute value to tune on mean-absolute-deviation\n",
    "    # Alternatively, could use:\n",
    "    # return resid **2-S\n",
    "    # for tuning on mean-squared-error\n",
    "    \n",
    "    return abs(resid)\n",
    "\n",
    "minimize(lambda_search_func)    # --> about 1.4957957957957957"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Implementing a k-folds cross-validation strategy will come in later assignments.\n",
    "\n",
    "#### Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Why is cross-validation useful?\n",
    "### 'a') to minimize the liklihood of overfitting\n",
    "### 'b') Cross-validation allows us to fit on all our data\n",
    "### 'c') cross-validation standardizes outputs\n",
    "### 'd') cross-validation is not useful\n",
    "### assing the character associated with your choice as a string to ans1\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "ans1 = 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 8",
     "locked": true,
     "points": "2",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<a id = \"sklearn\"></a>\n",
    "\n",
    "### Ridge Regression in `sklearn`  \n",
    "\n",
    "Below gives the syntax for implementing ridge regression in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "\n",
    "### Note, the \"alpha\" parameter defines regularization strength.\n",
    "### Lambda is a reserved word in `Python` -- Thus \"alpha\" instead\n",
    "\n",
    "### An alpha of 0 is equivalent to least-squares regression\n",
    "lr = LinearRegression()\n",
    "reg = Ridge(alpha = 100000)\n",
    "reg0 = Ridge(alpha = 0)\n",
    "\n",
    "# Notice how the consistent sklearn syntax may be used to easily fit many kinds of models\n",
    "for m, name in zip([lr, reg, reg0], [\"LeastSquares\",\"Ridge alpha = 100000\",\"Ridge, alpha = 0\"]):\n",
    "    \n",
    "    m.fit(data[['GrLivArea','YearBuilt']], data['SalePrice'])\n",
    "    print(name, \"Intercept:\", m.intercept_, \"Coefs:\",m.coef_,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Note, in the above example, an alpha of 100,000 is set for the ridge regularization. The reason an alpha value this high is required is because standardization / mean centering of our inputs did not occur, and instead of working with inputs on the order of [-4,4] we are on the interval of [0,2000].  \n",
    "\n",
    "#### Question 9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Above, the coefficent around 95/96 corresponds with:\n",
    "### 'a') Living Area\n",
    "### 'b') Year Built\n",
    "### 'c') Sale Price\n",
    "### Assign character associated with your choice as string to ans1\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "ans1 = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 9",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### Queston 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### True or False:\n",
    "### A larger \"alpha\" corresponds to a greater amount of regularization\n",
    "### assign boolean choice to ans1\n",
    "\n",
    "ans1 = 'False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 10",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
